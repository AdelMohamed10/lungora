import torch
import torch.nn.functional as F
from flask import Flask, request, jsonify
from PIL import Image
import io
import os
import gdown
from torchvision import transforms
import traceback
import logging

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Model download details
file_id = "15jLV7id21txc_sBZ_rJfsFJZEwB3QYiM"
model_url = f"https://drive.google.com/uc?id={file_id}"
model_path = "XRay_Classifier.pth"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = None
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Class labels
class_names = ["Covid", "Normal", "Pneumonia"]

descriptions = {
    "Covid": "ğŸ¦  ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ (COVID-19) Ù‡Ùˆ Ù…Ø±Ø¶ ÙÙŠØ±ÙˆØ³ÙŠ ÙŠØµÙŠØ¨ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ØªÙ†ÙØ³ÙŠ. Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ØªØ´Ù…Ù„ Ø§Ù„Ø­Ù…Ù‰ØŒ Ø§Ù„Ø³Ø¹Ø§Ù„ Ø§Ù„Ø¬Ø§ÙØŒ ÙˆØ¶ÙŠÙ‚ Ø§Ù„ØªÙ†ÙØ³. ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø¹Ø²Ù„ Ø§Ù„Ù…Ù†Ø²Ù„ÙŠ ÙˆØ§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©.",
    "Normal": "âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‡Ø§Ø¨ Ø±Ø¦ÙˆÙŠ Ø£Ùˆ Ø¹Ø¯ÙˆÙ‰. ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ù…Ø· Ø­ÙŠØ§Ø© ØµØ­ÙŠØŒ ÙˆØ´Ø±Ø¨ Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ØŒ ÙˆØ§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø¹Ù†Ø¯ Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠØ©.",
    "Pneumonia": "ğŸ« Ø§Ù„Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙˆÙŠ Ù‡Ùˆ Ø¹Ø¯ÙˆÙ‰ ØªØµÙŠØ¨ Ø§Ù„Ø±Ø¦ØªÙŠÙ†ØŒ ÙˆÙ‚Ø¯ ØªØ³Ø¨Ø¨ Ø§Ù„Ø³Ø¹Ø§Ù„ Ù…Ø¹ Ø§Ù„Ø¨Ù„ØºÙ…ØŒ Ø§Ù„Ø­Ù…Ù‰ØŒ ÙˆØ£Ù„Ù…Ù‹Ø§ ÙÙŠ Ø§Ù„ØµØ¯Ø±. ÙŠÙÙØ¶Ù„ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨."
}


def load_model():
    """Load the model from local storage or Google Drive."""
    global model
    try:
        if not os.path.exists(model_path):
            logger.info("â³ Downloading model from Google Drive...")
            gdown.download(model_url, model_path, quiet=False)
            logger.info("âœ… Model downloaded successfully!")

        logger.info(f"Loading model on {device}")
        model = torch.load(model_path, map_location=device)
        logger.info(f"âœ… Loaded model: {model}")

        model.eval()
        logger.info("âœ… Model ready for inference")
        return True
    except Exception as e:
        logger.error(f"âŒ Error loading model: {str(e)}")
        traceback.print_exc()
        return False


def preprocess_image(image_bytes):
    """Preprocess image before making predictions."""
    try:
        img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        return img_tensor
    except Exception as e:
        logger.error(f"âŒ Error processing image: {str(e)}")
        traceback.print_exc()
        raise ValueError("Failed to process input image")


@app.route("/", methods=["GET"])
def home():
    """API Home endpoint."""
    status = "Ù…ØªØ§Ø­" if model is not None else "ØºÙŠØ± Ù…ØªØ§Ø­ (Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)"
    return jsonify({
        "message": "Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© ØªØµÙ†ÙŠÙ ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ©!",
        "status": status,
        "classes": class_names
    })


@app.route("/predict", methods=["POST"])
def predict():
    """Predict the class of the uploaded X-ray image."""
    if model is None:
        if not load_model():
            return jsonify({"error": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠÙ‹Ø§"}), 503

    try:
        if "image" not in request.files:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ø£ÙŠ ØµÙˆØ±Ø©"}), 400

        image_bytes = request.files["image"].read()
        if not image_bytes:
            return jsonify({"error": "Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© ÙØ§Ø±ØºØ©"}), 400

        inputs = preprocess_image(image_bytes)

        with torch.no_grad():
            outputs = model(inputs)
            if hasattr(outputs, 'logits'):
                outputs = outputs.logits
            elif isinstance(outputs, (list, tuple)):
                outputs = outputs[0]

            logger.info(f"ğŸ” Output shape: {outputs.shape}")
            logger.info(f"ğŸ” Output content: {outputs}")

            probabilities = F.softmax(outputs, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)

        confidence_score = confidence.item()
        predicted_label = class_names[predicted_class.item()]
        all_probabilities = {class_names[i]: float(probabilities[0][i]) for i in range(len(class_names))}

        response = {
            "prediction": predicted_label,
            "confidence": f"{confidence_score:.2f}",
            "all_probabilities": all_probabilities
        }

        if confidence_score < 0.7:
            response["message"] = "âš ï¸ ØºÙŠØ± Ù…ØªØ£ÙƒØ¯! ÙŠÙØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„Ø·Ø¨ÙŠØ¨."
        else:
            response["info"] = descriptions[predicted_label]

        return jsonify(response)

    except ValueError as ve:
        logger.error(f"Input error: {str(ve)}")
        return jsonify({"error": str(ve)}), 400
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        traceback.print_exc()
        return jsonify({"error": "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ"}), 500


# Load the model on startup
try:
    load_model()
except Exception as e:
    logger.error(f"Failed to load model on startup: {str(e)}")
    traceback.print_exc()

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8081))
    app.run(port=port, debug=False, use_reloader=False)